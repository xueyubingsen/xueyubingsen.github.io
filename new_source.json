{
  "nodes": [
    {
      "id": "Page-00",
      "label": "305故障案例",
      "details": "305故障案例",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-01",
      "label": "TiDB大规模删除实践",
      "details": "TiDB大规模删除实践<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "https://tidb.net/blog/b7a90f87",
      "picture": ""
    },
    {
      "id": "Config-13",
      "label": "soft-pending-compaction-bytes-limit",
      "details": "soft-pending-compaction-bytes-limit<br>默认值192GB，达到阈值则触发write stall，客户端写入速度放慢",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01,Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-14",
      "label": "hard-pending-compaction-bytes-limit",
      "details": "hard-pending-compaction-bytes-limit<br>默认1024GB，达到阈值之后，不让客户端写入数据",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01,Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-15",
      "label": "max-merge-region-size",
      "details": "默认值54MB，控制Region Merge的size上限，当 Region key 大于指定值时 PD 不会将其与相邻的 Region 合并。<br><br>当region merge 速度慢的时候，考虑是否这个参数不当。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file/#max-merge-region-size",
      "picture": ""
    },
    {
      "id": "Config-16",
      "label": "max-merge-region-keys",
      "details": "控制 Region Merge 的 key 上限，当 Region key 大于指定值时 PD 不会将其与相邻的 Region 合并。<br>默认：540000。在 v8.4.0 之前，默认值为 200000；从 v8.4.0 开始，默认值为 540000。<br><br>当region merge 速度慢的时候，考虑是否这个参数不当。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file/#max-merge-region-keys",
      "picture": ""
    },
    {
      "id": "Config-17",
      "label": "split-merge-interval",
      "details": "split-merge-interval",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-01",
      "label": "compaction pending bytes",
      "details": "compaction pending bytes<br>TiKV-Details->RocksDB KV/RocksDB raft<br>等待进行Compaction的量的大小，<br>L1～Ln层待Compaction的SST文件大小过大，并且达到了阈值时，此指标会上涨",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-01",
      "label": "gc.max-write-bytes-per-sec",
      "details": "gc.max-write-bytes-per-sec",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-02",
      "label": "Commit Token Wait Duration",
      "details": "Commit Token Wait Duration",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-02",
      "label": "305 故障排除案例学习\n -计划内单机停机",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "Page-03",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-02",
      "label": "max-store-down-time",
      "details": "系统补副本操作的等待store恢复时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-03",
      "label": " miss-peer-region-count",
      "details": "grafana PD->PD dashboard->Region Health<br>单机维护期间，预期是一直下降",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-04",
      "label": "down-peer-region-count",
      "details": "grafana PD->PD dashboard->Region Health<br>单机维护期间，预期是0",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-05",
      "label": "pending-peer-region-count",
      "details": "grafana PD->PD dashboard->Region Health<br>单机维护期间，预期是一直下降",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-06",
      "label": "empty-region-count",
      "details": "grafana PD->PD dashboard->Region Health<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-07",
      "label": "Schedule operator create",
      "details": "grafana PD->Operator<br>发起Operator的监控<br>单机维护期间，miss-peer-region-count不下降的情况下观察这里<br><br>调度类型：balance根据指标（score）来调度。<br>remove-extra-replica，产生的remove extra replica的数量多少",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-43",
      "label": "Statistics-balance",
      "details": "Grafana->PD->Statistics-balance<br>监控每个TiKV的score的变化，TiDB v3.0 只和size有关，store-region-size，1MB=1分，pd-ctl store可以看到score和size一样的。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Index-07",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-44",
      "label": "Store availiable ratio",
      "details": "Grafana->PD->Statistics-balance<br>store的空间利用率监控<br>注意：达到了high-space-ratio参数值（老版本60%，新版本70%），此时TiDB的score计算公式会有变化，可能会导致score变化，进而引起PD调度<br>注意参数high-space-ratio和low-space-ratio",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-08",
      "label": "Operator finish duration",
      "details": "Operator 执行延迟",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-03",
      "label": "305 故障排除案例学习\n -计划外停机，满足多数派",
      "details": "305-TiDB 故障排除案例学习-计划外停机，满足多数派",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "Index-07,Index-08,Page-06",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-03",
      "label": "region-schedule-limit",
      "details": "同时进行 Region 调度的任务个数<br>默认值：2048<br><br>手工修复TiKV之前，设置次参数，不让pd发起调度<br>region merge速度慢的情况下，如果判断调度速度过慢，可以考虑调大此参数。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file/#region-schedule-limit",
      "picture": ""
    },
    {
      "id": "Config-04",
      "label": "replica-schedule-limit",
      "details": "手工修复TiKV之前，设置次参数，不让pd发起调度",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-05",
      "label": "leader-schedule-limit",
      "details": "手工修复TiKV之前，设置次参数，不让pd发起调度",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-06",
      "label": "merge-schedule-limit",
      "details": "默认值：8<br>同时进行的 Region Merge 调度的任务，设置为 0 则关闭 Region Merge。<br><br>手工修复TiKV之前，设置次参数，不让pd发起调度<br><br>region merge速度慢的情况下，如果判断调度速度过慢，可以考虑调大此参数。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file/#merge-schedule-limit",
      "picture": ""
    },
    {
      "id": "Error-01",
      "label": "SST file size mismatch",
      "details": "SST文件损坏，RocksDB Apply Snapshot",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-04",
      "label": "记一次sst文件损坏修复过程",
      "details": "记一次sst文件损坏修复过程<br>https://tidb.net/blog/54e388c8",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Error-02",
      "label": "Region is unavailable",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-05",
      "label": "如何处理损坏的sst文件",
      "details": "如何处理损坏的sst文件<br>https://tidb.net/blog/22731ef0",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Error-03",
      "label": "Raft 状态机器损坏",
      "details": "设置一个 Region 副本为 tombstone 状态",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-05",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/tikv-control/?_gl=1*1sh7tol*_gcl_au*MTYwMzk2MTYyOS4xNzYyOTMzNjQ2*_ga*MTYwNjU5NTgzNC4xNzYyOTMzNjQ2*_ga_3JVXJ41175*czE3NjQxMzY5ODMkbzEwJGcwJHQxNzY0MTM2OTk0JGo0OSRsMCRoMTc4NDU4Mzc4OQ..*_ga_ZEL0RNV6R2*czE3NjM5NzA2MTUkbzckZzEkdDE3NjM5NzA2NDEkajM0JGwwJGgw*_ga_CPG2VW1Y41*czE3NjQxMzY5ODMkbzEwJGcwJHQxNzY0MTM2OTgzJGo2MCRsMCRoMA..#%E8%AE%BE%E7%BD%AE%E4%B8%80%E4%B8%AA-region-%E5%89%AF%E6%9C%AC%E4%B8%BA-tombstone-%E7%8A%B6%E6%80%81",
      "picture": ""
    },
    {
      "id": "Error-04",
      "label": "last index",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-06",
      "label": "305 故障排除案例学习\n -计划外停机，不满足多数派",
      "details": "305 故障排除案例学习 <br>-计划外停机，不满足多数派<br><br>坏1个region有可能整个系统不可用，取决于region中存放的是什么数据<br>1、无法选举新Leader以及宕机超过max-store-down-time也无法进行补副本操作<br>2、前端业务访问部分出现报错<br>3、仅访问到异常Region的应用会出现报错<br><br>注意：<br>要先处理2副本丢失的region，处理完了再处理3副本丢失的region",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "Config-03,Config-04,Config-05,Config-06,Page-07",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-02",
      "label": "2副本丢失处理方式",
      "details": "2副本丢失处理方式，有损恢复<br>注意：<br>1、禁止前端业务写入，禁用相关调度<br>2、统计副本数丢失一半及以上region<br>3、确认region所属的数据库对象<br>4、unsafe-recover remove-fail-stores有损恢复<br>5、开启调度<br>6、确认region健康状态，数据校验",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-06",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-03",
      "label": "3副本丢失处理方式",
      "details": "3副本丢失处理方式<br>数据肯定丢失，恢复的目标是确保实例拉起<br>PD里面有丢失region的信息<br>1、关闭在线TiKV<br>2、创建空region<br>3、从应用侧将数据重新导入<br>4、数据与索引一致性检验<br>admin check index tbl_name idx_name",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-06",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-07",
      "label": "305 故障排除案例学习\n -计划外停机，不满足多数派，2副本丢失案例",
      "details": "305 故障排除案例学习<br> -计划外停机，不满足多数派，2副本丢失案例",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Note-02",
      "target": "Page-08",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-04",
      "label": "查看副本数",
      "details": "pg-ctl config show replication -u xxx",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Note-05",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-05",
      "label": "确认store状态",
      "details": "pg-ctl -u xxx store --jq=\".stores[].store|{id,address,state_name}\"",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Note-06",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-06",
      "label": "记录当前的调度规则",
      "details": "pg-ctl config show -u xxx|grep schedule-limit",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Note-07",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-07",
      "label": "禁止前端业务写入，禁止调度",
      "details": "pg-ctl -u xxx -i<br>config set region-schedule-limit 0<br>config set leader-schedule-limit 0<br>config set replica-schedule-limit 0<br>config set merge-schedule-limit 0<br>config set hot-region-schedule-limit 0<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Config-03,Config-04,Config-05,Config-06,Note-08",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-08",
      "label": "统计副本数丢失一半及以上的region",
      "details": "pg-ctl -u xxx region --jq='.regions[]|{id: .id,peer_stores: [.peers[].store_id]|select(length as $total | map(if .==(1,4,6) then . else empty end) | length>=$total-length)}'<br>1,4,6是故障节点的storeid<br>会打印出来一半以上的peer都在故障节点的region",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Note-09,Note-11",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-09",
      "label": "确认问题region所属数据库对象",
      "details": "select * from tikv_region_status where region_id in ('','','');",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "Note-10",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-10",
      "label": "2副本丢失处理：有损恢复",
      "details": "使用--all-regions在所有未发生异常的实例上remove-fail-stores<br>#关闭存活的TiKV节点<br>cd ${deploy}/scripts<br>./stop_tikv.sh<br># 在存活的TiKV节点执行<br>tikv-ctl --db /path/to/tikv-data/db unsafe-recover remove-fail-stores -s 1,4,6 --all-regions<br>tikv-ctl --db /path/to/tikv-data/db unsafe-recover remove-fail-stores -s 1,4,6 --all-regions<br>tikv-ctl --db /path/to/tikv-data/db unsafe-recover remove-fail-stores -s 1,4,6 --all-regions<br>执行之后的效果：告诉PD，出问题的region，凡是副本在此3个故障storeid上的region失效，<br>如果leader还在，则在发起命令的机器（可用节点）上重建副本，<br>如果leader都没有只剩下一个follower在可用节点上，那么这个follower变成leader，并且在可用节点上创建新的副本。<br>执行完之后启动可用节点的TiKV<br><br>注意：坏的老的region要做缩容处理，不要在集群的情况下启动起来，否则pd管理的混乱",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-07",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-08",
      "label": "305 故障排除案例学习\n -计划外停机，不满足多数派，3副本丢失案例",
      "details": "305 故障排除案例学习<br>-计划外停机，不满足多数派，3副本丢失案例<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Note-03",
      "target": "Note-04,Note-05,Note-06",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-11",
      "label": "统计副本全丢的region",
      "details": "pg-ctl -u xxx region --jq '.regions[] | select(has(\"leader\") | not) | {id: .id, peer_stores: [.peers[].store_id]}'<br>列出来的全是丢失3副本的region",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "Note-12",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-12",
      "label": "创建空region",
      "details": "#关闭存活的TiKV节点<br>cd ${deploy}/scripts<br>./stop_tikv.sh<br># 在关闭的存活的其中的2个TiKV节点<br>tikv-ctl --db /path/to/tikv-data/db recreate-region -p xxx(pd) -r ${region_id}<br>tikv-ctl --db /path/to/tikv-data/db recreate-region -p xxx(pd) -r ${region_id}",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "Note-15",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-13",
      "label": "从应用侧重新倒入数据（可选）",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-14",
      "label": "如果不可用的region是索引，则重建索引",
      "details": "admin check index tbl_name idx_name",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-15",
      "label": "开启PD调度",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "Note-07",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-16",
      "label": "确认region健康状态",
      "details": "定期检测集群不足3副本的region情况<br>pg-ctl -u xxx region --jq=\".regions[] | {id: .id,peer_stores: [.peers[].store_id] | select(length != 3)}\"",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-08",
      "target": "Index-03,Index-04,Index-05",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-09",
      "label": "305 故障排除案例学习\n -网络隔离",
      "details": "305-TiDB 故障排除案例学习-网络隔离",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "Page-10",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-17",
      "label": "2个IDC间出现网络隔离",
      "details": "3个IDC，其中2个IDC间出现网络隔离<br>TiKV不会出现大面积Leader Drop现象<br>TiDB server可能会报错<br>（如果每个IDC都有一个TiDB Server，则出错的概率是2/9）<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-09",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-18",
      "label": "信息孤岛",
      "details": "3个IDC，其中某个IDC出现网络隔离，形成信息孤岛<br>TiKV会出现大面积Leader Drop现象<br>TiDB server一定会报错",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-09",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-10",
      "label": "305 故障排除案例学习\n -TiKV Server is busy故障处理",
      "details": "305 故障排除案例学习<br> -TiKV Server is busy故障处理<br><br>原因有2个：<br>Write stall<br>Scheduler",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Error-5",
      "label": "Server is busy",
      "details": "当写入TiKV节点的时候，发现TiKV节点不可用<br>TiKV 节点写入压力过大，会触发TiKV的流控，Write stall",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-09",
      "label": "Server is busy",
      "details": "TiKV-Details->Errors->Server is busy<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-01",
      "label": "Write stall",
      "details": "RocksDB中写入量巨大，导致触发流控<br><br>1、当memtable文件数量过多，且达到阈值<br>2、L0层SST文件数量过多，且达到阈值（合并和压缩的速度慢于大量写入的速度导致）<br>3、L1～Ln层待Compaction的SST文件大小过大，并且达到了阈值<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-02",
      "label": "Scheduler",
      "details": "scheduler线程组，负责处理写入冲突检测，latch实现，先来后到的原则<br>过多的写请求处理会让scheduler触发流控",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Theory-01",
      "label": "RocksDB的写入流程",
      "details": "1、先写wal日志，持久化到RocksDB raft中<br>2、写请求写入到内存中的memtable，任何dml都是put操作，<br>memtable达到128MB之后，转储到immutable memtable，memtable供新的写入使用，<br>当immutable memtable 达到5个之后，就会将immutable memtable写入到sst文件中，<br>写入到sst文件（level 0）之后，wal中的日志就可以被覆盖了。<br>将写入的文件分了多层，level 0 和immutable memtable是一样的，<br>level 0的SST中，单个文件是有序的，文件之间是无序的，<br>此种情况对读操作极不优化，所以level 1及之后的sst中，会做一个大的排序和压缩，<br>所以level 1及之后的sst中，文件之间也都是有序的.<br><br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-03",
      "label": "当memtable文件数量过多，且达到阈值",
      "details": "TiKV以put,delete的形式写入到memtable<br>memtable受到write_buffer_size参数控制，默认128MB<br>lock_buffer_size，32MB<br>max_write_buffer_number，默认是5，积压的immutable memtable的数量<br>max-background-flushes，默认是2，immutable memtable刷到磁盘的线程数，搬运工<br>磁盘压力不大的情况下，调大搬运工<br>磁盘压力大的情况下：<br>调大immutable memtable的大小，write_buffer_size<br>调大max_write_buffer_number数量，让内存多容纳一些<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-10",
      "label": "memtable_count_limit_stop",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-11",
      "label": "memtable_count_limit_slowdown",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Log-01",
      "label": "stalling",
      "details": "rocksDB raft和kv中，搜stalling",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-06",
      "label": "write-buffer-size",
      "details": "默认128MB，memtable的大小",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-07",
      "label": "max-write-buffer-number",
      "details": "默认是5，积压的immutable memtable的数量",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-08",
      "label": "max-background-flushes",
      "details": "默认是2，immutable memtable刷到磁盘的线程数，搬运工",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-03",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-04",
      "label": "L0层SST文件数量过多，且达到阈值",
      "details": "level0写入到level1的过程流控参数<br>level0-file-num-compaction-trigger,默认是4，达到4个文件就开始合并排序<br>level0-slowdown-writes-trigger,默认是20，触发write stall的条件<br>level0-stop-writes-trigger,默认是36，停止写入，优先做合并<br><br>level0写入到level1，受到如下参数影响<br>磁盘压力不大情况下<br>rocksdb.max-sub-compactions，默认是3，分成3份做合并，调大<br><br>磁盘压力大的情况下<br>调大<br>level0-slowdown-writes-trigger<br>level0-stop-writes-trigger",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-01",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-12",
      "label": "level0_file_limit_stop",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-13",
      "label": "level0_file_limit_slowdown",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "Log-01",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-09",
      "label": "rocksdb.max-sub-compactions",
      "details": "rocksdb.max-sub-compactions，默认是3，分成3份做合并，调大",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-10",
      "label": "level0-file-num-compaction-trigger",
      "details": "level0-file-num-compaction-trigger,默认是4，达到4个文件就开始合并排序",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-11",
      "label": "level0-slowdown-writes-trigger",
      "details": "level0-slowdown-writes-trigger,默认是20，触发write stall的条件",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-12",
      "label": "level0-stop-writes-trigger",
      "details": "level0-stop-writes-trigger,默认是36，停止写入，优先做合并",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-04",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-05",
      "label": "L1～Ln层待Compaction的SST文件大小过大，并且达到了阈值",
      "details": "L1到Ln逐层向下压缩，压缩的条件：<br>每一层的数据了达到一个值之后才会向下进行压缩<br>读取L2的每个文件的最大最小值，扫描L3，<br>如果重合，则向下合并<br>如果L2有delete操作，则L3直接丢弃<br><br>rocksdb.defaultcf.soft-pending-compaction-bytes-limit,默认值192GB，达到阈值则触发write stall，客户端写入速度放慢<br>rocksdb.defaultcf.hard-pending-compaction-bytes-limit,默认1024GB，客户端不让写入<br>磁盘压力大的情况<br>rocksdb.rate-bytes-per-sec，默认是10GB，每秒写入的数据量<br>rocksdb.max-background-jobs,默认是8，线程数，可以调大<br><br>磁盘压力大的情况下<br>让上层多承担一些<br>调大rocksdb.defaultcf.soft-pending-compaction-bytes-limit<br>rocksdb.defaultcf.hard-pending-compaction-bytes-limit<br><br>压缩算法<br>compression-per-level",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-01",
      "target": "Index-01",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-14",
      "label": "pending_compaction_bytes_stop",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-15",
      "label": "pending_compaction_bytes_slowdown",
      "details": "TiKV-Details->RocksDB KV/RocksDB raft->Write stall reason",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-17",
      "label": "Compaction flow",
      "details": "Compaction的流量，会上涨",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-18",
      "label": "compression-per-level",
      "details": "每一层默认压缩算法。<br>defaultcf 的默认值：[\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]<br>writecf 的默认值：[\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]<br>lockcf 的默认值：[\"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"]",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-19",
      "label": "storage.flow-control",
      "details": "v5.2.0之后引入<br>代替RocksDB write stall流控机制<br>通过在TiKV scheduler层进行流控而不是在RocksDB层进行流控<br>改善流控算法，有效降低大写入压力下导致QPS下降的问题",
      "originalSize": 50.0,
      "originalDegree": 20.0,
      "source": "Event-05",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-06",
      "label": "Scheduler is too busy",
      "details": "scheduler pool里面有一个队列，请求越多队列越长，<br>通过内存轻量级锁latch来解决相同key的操作争用问题<br>sched_pending_write_threshold，默认值100MB，达到阈值之后，scheduler线程报错Scheduler is too busy<br>进而引起引Server is busy报错<br><br>解决：<br>autocommit改造为使用显示悲观事务<br>应用端改造调整业务实现方式，避免高并发key冲突",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-02",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-10",
      "label": "写写冲突",
      "details": "scheduler线程有一个线程池，由参数scheduler-pending-write-threshold决定，默认值100MB，达到阈值之后，scheduler线程报错Scheduler is too busy<br>进而引起引Server is busy报错<br>乐观锁才会比较容易造成scheduler的队列过长<br>悲观锁的锁等待不是在scheduler的latch里面<br>所有的自动提交默认都是乐观锁。所以自动提交的大量事务也会造成scheduler的队列过长",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-06",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-20",
      "label": "scheduler-pending-write-threshold",
      "details": "默认值：100MiB<br>写入数据队列的最大值，超过该值之后对于新的写入 TiKV 会返回 Server Is Busy 错误。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-18",
      "label": "QPS",
      "details": "TiDB->Query Summary ",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-19",
      "label": "Scheduler worker CPU",
      "details": "TiKV-Details->Thread CPU->Scheduler worker CPU<br>每一条线就是一个TiKV，如果有某个TiKV格外繁忙，说明产生了热点",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-20",
      "label": "Scheduler latch wait duration",
      "details": "TiKV-Details->Scheduler->Scheduler latch wait duration<br>可以间接看出队列的长度",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-21",
      "label": "Scheduler pending commands",
      "details": "TiKV-Details->Scheduler <br>每个 TiKV 实例上 pending 命令的个数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-22",
      "label": "Scheduler writing bytes",
      "details": "TiKV-Details->Scheduler <br>等待写入的数据流量",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-23",
      "label": "Failed Query OPM",
      "details": "TiDB->Query Summary->Failed Query OPM<br>每个 TiDB 实例上，对每分钟执行 SQL 语句发生的错误按照错误类型进行统计（例如语法错误、主键冲突等）。包含了错误所属的模块和错误码。<br>写写冲突太多的监控，日志中伴随着会有大量的9007错误<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-24",
      "label": "KV Backoff OPS",
      "details": "TiDB->KV Errors->KV Backoff OPS<br>TiKV 返回错误信息的数量<br><br>txnLock多的话，提示写写冲突太多<br>要么用了大量的乐观锁",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Log-02",
      "label": "grep conflict",
      "details": "tidb.log中过滤conflict关键词，确认哪些key和语句导致的冲突<br>TiDB Dashboard SQL语句分析执行详情页面/慢查询页面",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-10",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-07",
      "label": "TiKV集群写入慢",
      "details": "TiKV集群写入慢也会导致Scheduler is too busy",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-06",
      "target": "Page-24",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-08",
      "label": "网络故障",
      "details": "网卡丢包<br>ring buffer打满问题<br>tcp缓冲池打满<br>网卡绑定的问题，2个不同品牌的网卡绑定会有问题",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-06",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-25",
      "label": "Server report failures",
      "details": "TiKV-Details->Errors",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-08",
      "target": "Log-03",
      "link": "",
      "picture": ""
    },
    {
      "id": "Log-03",
      "label": "过滤tikv.log",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-08",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-11",
      "label": "305-TiDB 故障排除案例学习-TiKV Leader Drop故障处理",
      "details": "305-TiDB 故障排除案例学习<br>-TiKV Leader Drop故障处理<br><br>现象：<br>Leader drop升高<br>TiClient Region Error OPS升高<br>客户查询延迟升高,Duration升高<br>Leader drop错误升高<br><br>可能原因：<br>raft group超时选举<br>PD调度<br><br>定位：<br>TiKV-Details->Raft message->Messages<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-26",
      "label": "Leader drop",
      "details": "TiKV-Details->Errors->Leader drop<br>每个TiKV实例上drop leader的个数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-27",
      "label": "TiClient Region Error OPS",
      "details": "TiDB->KV Errors->TiClient Region Error OPS<br>region相关错误信息数量，leader发生切换之后，客户端发生的错误，not_leader升高 ",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-28",
      "label": "Leader drop",
      "details": "TiKV-Trouble-Shooting->Leader Drop->Leader",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-29",
      "label": "Duration",
      "details": "TiDB Dashboard->Query Summary->Duration<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-09",
      "label": "raft group超时选举",
      "details": "raftstore pool负责处理写入<br>1、propose<br>2、apply<br>3、replicate<br><br>每个region副本所在的节点都会发送心跳<br>raftstore pool同时也负责心跳信息处理<br><br>如果没有收到心跳信息，则会发生leader 选举<br>1、网络繁忙<br>2、raftstore pool繁忙（写入繁忙）",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-15",
      "label": "PD调度",
      "details": "<br>PD调度<br>1、balance-leader<br>2、hot-region-scheduler<br>大部分情况下，都是因为节点发生故障导致发生节点选举超时，引起的PD调度<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "Index-07",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-30",
      "label": "Messages",
      "details": "TiKV-Details->Raft message->Messages<br>1、transfer_leader指标：PD调度参考的指标<br>2、prevote指标：超时选举参考的指标",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-11",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-14",
      "label": "raftgroup选举超时的原因",
      "details": "1、raftstore pool太繁忙<br>2、心跳发送太多<br>3、IO问题<br>4、TiKV集群写入慢<br>5、网络异常<br>6、TiKV发生重启",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-09",
      "target": "Event-07,Event-08",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-12",
      "label": "raftstore pool太繁忙",
      "details": "TiKV-Details->Thread CPU->Raft store CPU<br>70%～80%是正常值<br>可以调整这个参数，store-pool-size参数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-31",
      "label": "Raft store CPU",
      "details": "TiKV-Details->Thread CPU->Raft store CPU<br>70%～80%是正常值",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-12",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-11",
      "label": "心跳发送太多",
      "details": "减少Region的心跳发送，降低Raftstore心跳负担<br>Hibernate Region：特定Region减少发心跳频率<br>Region Merge：减少Region数量，间接减少心跳发送",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-12",
      "target": "Config-25",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-21",
      "label": "store-pool-size",
      "details": "单位是线程个数，默认2个线程，打满是200%",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-12",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-13",
      "label": "IO问题",
      "details": "查看指标Disk Latency",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-32",
      "label": "Disk Latency",
      "details": "Overview->System Info->IO util->Disk Latency",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-13",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-33",
      "label": "ping latency",
      "details": "Grafana->blackbox_exporter->ping latency",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-08",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-57",
      "label": "node_exporter",
      "details": "网络相关的在Network部分",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-08",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-16",
      "label": "TiKV 异常重启",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-34",
      "label": "Uptime",
      "details": "TiKV-Details->Cluster->Uptime",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-16",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-35",
      "label": "Store leader score",
      "details": "Grafana PD->Statistics - banalce->Store leader score/size/count<br>TiKV leader节点的分数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-36",
      "label": "Store leader size",
      "details": "Grafana PD->Statistics - banalce->Store leader score/size/count<br>TiKV leader节点的数量",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-37",
      "label": "Store leader count",
      "details": "Grafana PD->Statistics - banalce->Store leader score/size/count<br>TiKV leader节点的大小",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Event-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-12",
      "label": "305 故障排除案例学习-PD故障处理",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-13",
      "label": "不产生调度",
      "details": "现象：<br>扩容（增加节点），4扩5，扩容之后，等了很长时间没发生调度<br>调度生成的规则：<br>首先根据label来生成，pd-ctl store查看，发现2个store的labels一样。<br>总结：<br>扩容一定要注意维护label和隔离级别。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-12",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-14",
      "label": "调度过慢",
      "details": "现象：region health里面的<br>extra-peer-region-count一只在上升，不符合预期<br><br>grafana PD->PD dashboard->Region Health<br>集群内部进行region分裂，迁移等操作<br>某些低版本，如果使用了tiflash<br><br>1、产生多少remove调度<br>过滤PD leader日志中的split<br>如果多的化，说明产生了调度，继续排查执行调度的问题<br>如果少的化，说明PD没有启动调度来删除中间的peer<br><br>2、其他调度影响<br>对比其他调度的Schedule operator create和Schedule operator finish,发现balance-leader和balance-region的完成度都不太好，能确认是PD本身的问题。<br><br>3、排查PD的原因<br>PD的作用：生成TSO，提供数据字典，处理心跳，生成调度<br>Region heartbeat report有200个<br>Heartbeat region event QPS，有2000个<br>说明PD处理心跳已经力不从心了<br><br>结论：<br>heartbeat导致的PD CPU使用率较高，并出现了心跳更新瓶颈。<br><br>解决：<br>消除extra peer，修改trace-region-flow，没有读写的region，即使报了心跳，也会忽略。5.1之后，被参数flow-round-by-digit代替<br>开启Hibernate Region（静默region），滚动重启",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-12",
      "target": "Index-07",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-46",
      "label": "extra-peer-region-count",
      "details": "grafana PD->PD dashboard->Region Health<br>集群内部进行region分裂，迁移等操作<br>某些低版本，如果使用了tiflash<br>schedule.enable-remove-extra-replica参数关闭",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-47",
      "label": "remove-extra-replica",
      "details": "PD->Operator->Schedule operator create",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-48",
      "label": "Heartbeat region event",
      "details": "PD->Heartbeat region event<br>region很多的情况下，会处理大量的心跳信息，默认10s一次<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-49",
      "label": "Region heartbeat report",
      "details": "PD这边处理过的心跳数量，处理不过来的心跳信息会缓存到PD的缓存中。<br>这个值和TiKV Leader count的值做比对",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-50",
      "label": "Heartbeat region event QPS",
      "details": "PD->Heartbeat->Heartbeat region event QPS<br>update_cache多的话，说明缓存到PD内存中的心跳比较多",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-26",
      "label": "flow-round-by-digit",
      "details": "PD根据流量来判断是否处理心跳信息<br>PD 会对流量信息的末尾数字进行四舍五入处理，减少 Region 流量信息变化引起的统计信息更新。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-25",
      "label": "raftstore.hibernate-regions",
      "details": "TiKV的参数，静默region",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-14",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-15",
      "label": "频繁产生调度",
      "details": "现象：<br>集群版本v3.0，TiDB Server大量出现 not leader错误<br>QPS/TPS延迟升高<br><br>排查：<br>确认调度类型->排查对应调度类型的参数->分析产生原因（扩缩容，修改label，参数修改）<br><br>集群总region是30k，但是空region达到了20k。<br>假设空region是6w个，每个分数是1分（1MB），分数就是6w。<br>此时数据加载，100MB1个region，1000个region，共记10wMB，<br>这1000个region分布在3个TiKV上，每个TiKV都加了10W分，<br>但是region的个数只加了1000，count很低，分数很高，导致触发调度。<br><br>V3.0版本之后，新增参数：<br>leader-schedule-policy，可选值size/count，<br>region-weight，默认2<br>leader-weight，默认2<br><br>解决：<br>1、调大tolerant-size-retio，调整可容忍的比率，比如是10，<br>那么分数是十倍也是可以被容忍的，即十倍以内都不会调度。先调整这个参数停止频繁的调度。<br>2、开启region merge功能",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-12",
      "target": "Index-27,Index-07,Index-06",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-16",
      "label": "store leader score不均衡",
      "details": "现象：<br>开启hibernate-regions参数，滚动重启集群期间。中途中断后，发现store之间的leader分布始终处于不均衡状态。store1的leader数量一直很低。<br><br>排查思路：<br>调度有没有产生->调度产生情况->调度执行情况<br><br>PD的日志：<br>store1频繁发生调度，因为hibernate-regions参数开启，<br>不活跃的regions发送心跳频率降低，导致其他region发起选举，由于该region不发送心跳，PD认为该storeid的leader节点太少，重新发起调度。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-12",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-51",
      "label": "balance leader scheduler",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-16",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-38",
      "label": "Store region score",
      "details": "PD->Statistics-balance->Store region score<br>TiKV region的分数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-13",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-39",
      "label": "Store region size",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-13",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-40",
      "label": "Store region count",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-13",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-17",
      "label": "label和高可用",
      "details": "PD只能保证region不在同一个TiKV上，<br>默认不会识别zone，DC，rack，host等，<br>label的作用就是让PD可以感知到。<br><br>工作原理：<br>TiKV：每个TiKV都打一个标签<br>PD：指定location-labels和isolation-level<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-13",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-41",
      "label": "TiKV-Details->Cluster->region",
      "details": "TiKV-Details->Cluster->region<br>region数量从故障开始时间点比较剧烈波动",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-42",
      "label": "TiKV-Details->Cluster->leader",
      "details": "TiKV-Details->Cluster->leader<br>leader数量从故障开始时间点比较剧烈的波动",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-45",
      "label": "Number of Regions",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-22",
      "label": "leader-schedule-policy",
      "details": "PD的参数，可选值size/count",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-23",
      "label": "region-weight",
      "details": "TiKV的参数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-24",
      "label": "leader-weight",
      "details": "TiKV的参数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-15",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-18",
      "label": "PD故障案例-TSO生成慢",
      "details": "现象<br><br>诊断：<br>Start TSO Wait Duration 300ms<br>Start TSO Wait Duration 30ms<br>PD TSO Wait Duration 250ms<br>PD TSO OPS 数量有激增<br>PD Client CMD OPS 有激增<br><br>解决：<br>云主机TiDB Server混合部署，高并发下，CPU处理能力下降<br>分开部署<br><br>总结：<br>根据TSO的获取流程及对应监控项，定位获取tso慢的范围。<br>如果TiDB节点服务器PCU负载较高，可能会导致TSO的获取duration较高。<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Log-04",
      "label": "get timestamp too slow",
      "details": "TiDB日志中有get timestamp too slow的日志，说明TSO生成慢",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-19",
      "label": "TSO的工作原理",
      "details": "参考PD(Placement Driver)主要功能",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-52",
      "label": "Start TSO Wait Duration",
      "details": "TiDB->PD Client-> Start TSO Wait Duration<br>请求TSO到收到tsFuture的duration，只到PDclient，TiDB Server内部处理",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-53",
      "label": "PD TSO RPC Duration",
      "details": "TiDB->PD Client->PD TSO RPC Duration<br>PD client发出TSO请求到收到分配的TSO的时间，网络+PD处理",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-54",
      "label": "PD TSO Wait Duration",
      "details": "TiDB->PD Client->PD TSO Wait Duration<br>从进入tsFuture.wait到获得TSO的等待时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-55",
      "label": "PD Client CMD OPS",
      "details": "TiDB->PD Client<br>tso_async_wait有激增",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-56",
      "label": "PD TSO OPS",
      "details": "CMD永远大于request，因为PD Client有批处理的优化",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-18",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-19",
      "label": "PD故障案例-PD高可用异常",
      "details": "现象：<br>巡检发现PD日志中出现 sync duration of 3.0s, expected less than 1s告警信息<br>与leader/follower角色无关<br>每隔28天出现一次，持续3s左右<br>集群中各个主机伴随leader切换的行为（肯定有业务的延迟）<br><br>PD高可用异常定位思路<br><br>PD leader选举依赖内嵌的etcd选举，etcd也是raft选举机制。<br>etcd磁盘性能抖动也会导致选举<br>网络抖动：<br>（1）网络故障<br>（2）服务器负载高<br><br>磁盘抖动：<br>（1）etcd本身问题<br>etcd->99% Handle txn duration<br>etcd->99% Peer round trip time seconds<br>指标都在5ms<br>（2）磁盘问题<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "Event-08",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-58",
      "label": "etcd->99% Handle txn duration",
      "details": "PD->etcd",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-19",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-59",
      "label": "etcd->99% Peer round trip time seconds",
      "details": "PD->etcd",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-19",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-17",
      "label": "磁盘问题",
      "details": "磁盘空间<br>单享盘<br>io util<br>write latency<br>硬件问题<br>raid卡28天充放点一次。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-19",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-20",
      "label": "PD故障案例-扩缩异常",
      "details": "状态变化：online->offline->tombstone<br>region health->offline-peer-region-count<br><br>排查方向：<br>磁盘空间<br>label配置<br>违反raft协议<br><br>解决（手工下线）：<br>1、查看下线store还存在的region id<br>pg-ctl -d -u xxx region store 4(要下线的storeid)<br>2、使用pd-ctl transfer-region手工调度<br>3、所有region transfer之后状态自动变为tombstone<br><br>注意：<br>pd-ctl -d看不到learner角色，PD下线之前先创建1个learner角色，等learner角色日志追上之后，会变成follower<br><br>如下命令会打印出包括learner角色的storeid是4的所有的region信息<br>region -jq=\".regions[] | {id: .id,peer_stores: [.peers[].store_id] | select(any(.==4))}\"<br><br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-59",
      "label": "offline-peer-region-count",
      "details": "扩所容的时候，预期是不断下降",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-20",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-21",
      "label": "PD故障案例-TiKV状态异常",
      "details": "超过20s没收到TiKV发送的heartbeat之后，会把该TiKV标记为<br>Disconnected状态<br>网络<br>raftstore pool主要处理写请求，处理心跳<br>如果写请求延迟表达的化，说明raftstore 线程的确比较忙<br><br>总结：<br>store heartbeat的任务需要先发送tick 给raftstore，然后在发送heartbeat 给 pd，因为raftstore较忙，propose wait 很高，导致store heartbeat无法及时发送，PD节点发现20s未收到heartbeat就会标记TiKV未disconnected状态。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-00",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-60",
      "label": "Propose wait duration",
      "details": "TiKV-Details->Raft propose->Propose wait duration<br>从发送请求给Raftstore，到Raftstore真正开始处理请求之间的延迟时间。<br>如果2-3s，说明raftstore在线程池队列的比较多，都在排队<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-21",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-22",
      "label": "304 性能调优",
      "details": "304 性能调优",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-23",
      "label": "大量查询超时案例",
      "details": "核实情况：<br>TiDB->Query Summary->Duration<br>TiDB->Query Summary->QPS 排查是否有业务激增<br>TiDB->Query Summary->999/99/95/80 Duration 排查具体是写的问题还是读的问题<br>TiDB->Query Detail->Duration 999/99/95/80 by instance 排查是否是某一台的问题，如果不是某一台，大概率不是TiDB Server的问题<br>结合DML语句读写流程<br>（1）TSO获取快慢<br>TiDB->PD Client->PD TSO Wait/RPC Duration<br>（2）编译/解析<br>TiDB->Executor->Parse/Compile Duration<br>（3）排除backup off问题<br>TiDB->KV Errors->KV Backoff Duration/KV Backoff OPS 排除是Backup off的错误问题<br>（4）TiKV问题<br>TiDB->KV Request->KV Request Duration 99 by store 排查哪个TiKV有问题<br>TiDB->KV Request->KV Request Duration 99 by type 排除是那种类型的Request，BatchGet是pointGet的一种。<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-22",
      "target": "Index-52,Index-53,Index-54,Index-55,Index-56",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-24",
      "label": "写入频繁抖动",
      "details": "背景<br>v5.1.0，TiKV aws 4c32g+千兆+1TBnvme<br>9:30左右insert 写入频繁抖动，最高1s，集群资源（CPU，IP，网络）均未达到瓶颈<br><br>排查<br>TiKV-Details->gRPC->99% gRPC message duration排除掉网络问题，并且该指标有具体的写入类型，kv_prewrite慢<br>结合TiKV写入流程继续分析是线程的问题？还是IO的问题<br>写入线程问题指标：<br>TiKV-Details->Thread CPU->gRPC poll CPU<br>TiKV-Details->Thread CPU->Scheduler worker CPU<br>TiKV-Details->Thread CPU->Raft store CPU<br>TiKV-Details->Thread CPU->Async apply CPU<br>上述指标都正常<br><br>IO问题相关指标：<br>TiKV-Details->Raft IO->99% Apply log duration per server<br>TiKV-Details->Raft IO->99% Append log duration per server<br>TiKV-Details->Raft IO->99% Commit log duration per server 有1台TiKV指标升高<br>TiKV-Details->Raft propose->Propose wait duration per server 对应实例的Raft store线程队列也出现排队等待  <br>TiKV-Details->Raft propose->Apply wait duration per server<br><br>TiKV-Details->Raft Engine->Write Duration <br>Node_exporter->Disk 磁盘IO比较低，但是写入延迟上涨明显。<br><br>结论：<br>问题TiKV 磁盘有问题，换了一台云主机解决。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-22",
      "target": "Index-19,Index-31",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-60",
      "label": "99% gRPC message duration",
      "details": "TiKV-Details->gRPC->99% gRPC message duration<br>排查问题时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Note-20",
      "label": "TiKV数据写入流程",
      "details": "raftstore pool负责处理写入，同时也负责处理心跳信息<br>写入流程如下<br>（1）propose<br>（2）append<br>（3）replicate->remote append<br>（4）commit<br>（5）apply",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-61",
      "label": "gRPC poll CPU",
      "details": "TiKV-Details->Thread CPU->gRPC poll CPU",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-62",
      "label": "Async apply CPU",
      "details": "TiKV-Details->Thread CPU->Async apply CPU",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-63",
      "label": "99% Apply log duration per server",
      "details": "TiKV-Details->Raft IO->99% Apply log duration per server<br>每一条线是1个TiKV服务器，很方便的排查出是否是单个TiKV的磁盘问题，rocksdb Apply的平均时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-64",
      "label": "99% Append log duration per server",
      "details": "TiKV-Details->Raft IO->99% Append log duration per server<br>每一条线是1个TiKV服务器，很方便的排查出是否是单个TiKV的磁盘问题，rocksdb Append的平均时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-65",
      "label": "99% Commit log duration per server",
      "details": "TiKV-Details->Raft IO->99% Commit log duration per server<br>每一条线是1个TiKV服务器，很方便的排查出是否是单个TiKV的磁盘问题，rocksdb Apply的平均时间",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-66",
      "label": "99% Propose wait duration per server ",
      "details": "TiKV-Details->Raft propose->99% Propose wait duration per server<br>每一条线是1个TiKV服务器，很方便的排查出是否是单个TiKV的磁盘问题，Propose的平均时间，如果是IO慢，这里会随之变慢",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-67",
      "label": "99% Apply wait duration per server",
      "details": "TiKV-Details->Raft propose->99% Apply wait duration per server<br>每一条线是1个TiKV服务器，很方便的排查出是否是单个TiKV的磁盘问题，Propose的平均时间，如果是IO慢，这里会随之变慢",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-24",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-25",
      "label": "磁盘容量影响PD调度",
      "details": "现象：<br>TiDB Duration从11.10日开始突然不稳定，几个TiKV的region数量频繁变化<br><br>原理：<br>PD频繁调度会导致大量backoff报错，查询的流程变的复杂很多，进而引起查询时间抖动<br><br>排查：<br>PD->Statistics-balance->Store region score<br>PD->Scheduler->Balance region movement(老版本名Balance region event)<br>PD->Statistics-balance->Store availiable ratio 接近40%左右，达到了high-space-ratio参数值60%，此时TiDB的score计算公式会有变化，导致score一只上涨<br><br>PD产生调度的原因：<br>热点<br>磁盘容量：high-space-ratio",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-22",
      "target": "Index-38,Index-44",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-27",
      "label": "high-space-ratio",
      "details": "默认值：0.7。<br>设置 store 空间充裕的阈值。当节点的空间占用比例小于该阈值时，PD 调度时会忽略节点的剩余空间，主要根据实际数据量进行均衡。此配置仅在 region-score-formula-version = v1 时生效。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-25",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-28",
      "label": "low-space-ratio",
      "details": "默认值：0.8。<br>设置 store 空间不足的阈值。当某个节点的空间占用比例超过该阈值时，PD 会尽可能避免往该节点迁移数据，同时主要根据节点剩余空间大小进行调度，避免对应节点的磁盘空间被耗尽。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-25",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Index-69",
      "label": "Balance region movement",
      "details": "PD->Scheduler->Balance region movement(老版本名Balance region event)",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-25",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-26",
      "label": "UnifyRead Pool原理",
      "details": "TiDB 4.0 新 Feature 原理及实践：统一读线程池",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "https://tidb.net/blog/56f2a0cd",
      "picture": ""
    },
    {
      "id": "Config-37",
      "label": "use-unified-pool",
      "details": "是否使用统一的读取线程池（在 readpool.unified 中配置）处理存储请求。该选项值为 false 时，使用单独的存储线程池。通过本节 (readpool.storage) 中的其余配置项配置单独的线程池。<br>默认值：如果本节 (readpool.storage) 中没有其他配置，默认为 true。否则，为了升级兼容性，默认为 false，请根据需要更改 readpool.unified 中的配置后再启用该选项。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-26",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/tikv-configuration-file/#use-unified-pool",
      "picture": ""
    },
    {
      "id": "Config-38",
      "label": "auto-adjust-pool-size",
      "details": "是否开启自动调整线程池的大小。开启此配置可以基于当前的 CPU 使用情况，自动调整统一处理读请求的线程池 (UnifyReadPool) 的大小，优化 TiKV 的读性能。目前线程池自动调整的范围为：[max-thread-count, MAX(4, CPU)](上限与 max-thread-count 可设置的最大值相同)。",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-26",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-27",
      "label": "TiDB的变量类型",
      "details": "系统变量<br>（1）全局系统变量<br>set @@global.auto_increment_increment=10;<br>set global auto_increment_increment = 10;<br>select @@global.auto_increment_increment;<br>show global variables like 'auto_increment_increment';<br>（2）会话系统变量<br>set @@session.auto_increment_increment=10;<br>set session auto_increment_increment = 10;<br>select @@session.auto_increment_increment;<br>show session variables like 'auto_increment_increment';<br><br>用户变量<br>set @aaa='bb';<br>select @aaa;<br><br>服务器状态变量<br>show global status;<br>show session status;<br><br>配置文件参数<br>tiup cluster edit-config xxx<br>tiup cluster show-config xxx<br>注意：PD的参数修改有可能用edit-config不生效，需要用pg-ctl来修改",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Doc-01",
      "label": "SOP文档",
      "details": "平凯7.1.8 SOP",
      "originalSize": 50.0,
      "originalDegree": 20.0,
      "source": "",
      "target": "",
      "link": "https://pingcap-cn.feishu.cn/wiki/Y6Mzw7PVqiydiAkXGOOcpCsPnxc",
      "picture": ""
    },
    {
      "id": "Doc-02",
      "label": "参数规范",
      "details": "SOP-参数规范",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Doc-01",
      "target": "",
      "link": "https://pingcap-cn.feishu.cn/wiki/TusEwJBCDibl7wknBzFcV8pOnOY",
      "picture": ""
    },
    {
      "id": "Doc-03",
      "label": "微众银行参数规范",
      "details": "微众银行参数规范",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Doc-02",
      "target": "",
      "link": "https://pingcap-cn.feishu.cn/sheets/HSvwszXtFhZCSJtave7cozehnNf?sheet=SwcSZv",
      "picture": ""
    },
    {
      "id": "Page-28",
      "label": "统计信息管理",
      "details": "统计信息管理<br>直方图：等深直方图，Count-min Sekatch<br>影响统计信息收集的参数：<br>统计信息收集方法<br>analyze table $table <br><br>自动更新统计信息：<br>更新周期：20*stats lease，每60s更新一次统计信息<br>tidb_auto_analyze_ratio：当前修改行数/总行数<br>tidb_auto_analyze_start_time 默认1天都可以自动收集统计信息<br>tidb_auto_analyze_end_time<br><br>查看统计信息：<br>show analyze status;--统计信息收集状态<br>show stats_meta where db_name = '' and table_name = '';--表修改行数<br>show stats_healthy where db_name = '' and table_name = '';--健康度<br>健康度：表中未被修改的行数/总行数<br>show stats_histograms where db_name = '' and table_name = '';--列的元信息<br>show stats_buckets where db_name = '' and table_name = '';--直方图信息<br><br>导入导出统计信息方法<br>http://ip:status-port/stats/dump/db/table--导出统计信息<br>http://ip:status-port/stats/dump/db/table/yyyy-mm-dd hh24:mi:ss--导出某一时间的统计信息<br>load stats aaa.json;--导入统计信息<br>drop stats db.table;--删除统计信息<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-29",
      "label": "tidb_build_stats_concurrency",
      "details": "微众银行，使用默认值2",
      "originalSize": 30.0,
      "originalDegree": 8.0,
      "source": "Page-28",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/system-variable-reference/#tidb_build_stats_concurrency",
      "picture": ""
    },
    {
      "id": "Config-30",
      "label": "tidb_distsql_scan_concurrency",
      "details": "可以用于控制一次读取的Region数量",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-28",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/system-variable-reference/#tidb_distsql_scan_concurrency",
      "picture": ""
    },
    {
      "id": "Config-31",
      "label": "tidb_index_serial_scan_concurrency",
      "details": "可以用于控制一次读取的Region数量",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-28",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-32",
      "label": "stats-lease",
      "details": "TiDB 重载统计信息，更新表行数，检查是否需要自动 analyze，利用 feedback 更新统计信息以及加载列的统计信息的时间间隔。<br>默认值：3s<br>每隔 stats-lease 时间，TiDB 会检查统计信息是否有更新，如果有会将其更新到内存中<br>每隔 20 * stats-lease 时间，TiDB 会将 DML 产生的总行数以及修改的行数变化更新到系统表中<br>每隔 stats-lease 时间，TiDB 会检查是否有表或者索引需要自动 analyze<br>每隔 stats-lease 时间，TiDB 会检查是否有列的统计信息需要被加载到内存中<br>每隔 200 * stats-lease 时间，TiDB 会将内存中缓存的 feedback 写入系统表中<br>每隔 5 * stats-lease 时间，TiDB 会读取系统表中的 feedback，更新内存中缓存的统计信息<br>当 stats-lease 为 0s 时，TiDB 会以 3s 的时间间隔周期性的读取系统表中的统计信息并更新内存中缓存的统计信息。但不会自动修改统计信息相关系统表，具体来说，TiDB 不再自动修改这些表：<br>mysql.stats_meta：TiDB 不再自动记录事务中对某张表的修改行数，也不会更新到这个系统表中<br>mysql.stats_histograms/mysql.stats_buckets 和 mysql.stats_top_n：TiDB 不再自动 analyze 和主动更新统计信息<br>mysql.stats_feedback：TiDB 不再根据被查询的数据反馈的部分统计信息更新表和索引的统计信息",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-28",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file/#stats-lease",
      "picture": ""
    },
    {
      "id": "Page-29",
      "label": "基于索引的SQL优化",
      "details": "索引管理方法<br>- online ddl <br>admin show ddl jobs;<br>增加索引的原理：<br>创建元数据<br><br>回填数据<br><br><br>增加索引对线上业务的影响<br><br>索引扫描的方式<br>唯一索引或者主键的情况下，最优的选择<br>Point Get<br>Batch Point Get<br>Index Full Scan<br>Index Range Scan<br><br>索引选择的规则<br>索引选择的维度：<br>索引列涵盖了多少访问条件<br>索引是否需要回表：indexReader不需要回表，IndexLookupReader是需要会表<br>索引是否满足一定顺序<br><br>基于代价的选择：<br>索引的每行数据在存储层的平均长度<br>索引生成的查询范围的行数<br>索引的回表代价<br>索引查询时的范围数量<br><br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-33",
      "label": "tidb_ddl_reorg_worker_cnt",
      "details": "可以动态修改：",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-29",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-34",
      "label": "tidb_ddl_reorg_batch_size",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "Page-29",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "",
      "label": "SQL优化实战",
      "details": "快速定位到有问题的SQL<br>高频语句是优化的重点<br><br>DML语句如何优化 <br><br>基于执行计划的优化",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-33",
      "label": "TiDB server关键性能参数与优化",
      "details": "操作系统参数<br>CPU:<br>Dynamic frequency scaling :<br>定义：CPU动态节能技术用于降低服务器功耗，通过选择系统空闲状态不同的电源管理策略，可以实现不同程度降低服务器功耗，更低的功耗策略意味着CPU唤醒更慢对性能影响更大。<br>需要调整为：performance<br><br>Numa Binding:<br>定义：内存直接绑定在CPU上，CPU只有访问自身管理的内存物理地址时，才会有较短的响应时间。<br>NUMA架构的服务器，图弄过绑定NUMA节点，尽可能的避免跨NUMA访问内存，提升tidb-server的性能。<br><br>内存：<br>THP关掉。<br>VM parameters<br>dirty_retio,dirty_background_ratio:通常不需要调整，对于高性能SSD，降低其值有利于提高内存回收时的效率。<br><br>系统参数 <br><br>集群参数",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "",
      "label": "tidb_mem_quota_query",
      "details": "默认值：",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-30",
      "label": "分页查询",
      "details": "分页查询最佳实践",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/dev-guide-paginate-results/#%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2",
      "picture": ""
    },
    {
      "id": "Page-31",
      "label": "TiDB OOM 故障排查",
      "details": "TiDB OOM 故障排查",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/troubleshoot-tidb-oom/#tidb-oom-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5",
      "picture": ""
    },
    {
      "id": "Config-35",
      "label": "Connection Idle Duration",
      "details": "TPS压不上去，可以观察这个指标，如果太长，可能是应用端的问题",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "https://docs.pingcap.com/zh/tidb/stable/dashboard-monitoring/#connection-idle-duration",
      "picture": ""
    },
    {
      "id": "",
      "label": "transaction_isolation",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-36",
      "label": "max-proc",
      "details": "默认值0，表示使用机器上所有的CPU<br>如果是混合部署，要相应设置这个参数<br>另外云主机扩容CPU之后，要重启tidb server才能使这个参数生效。<br><br>设置的比实际cpu数量高会引起生成执行计划慢等性能问题<br>https://pingcap-cn.feishu.cn/wiki/L1xYww5pmiS2PekbGAMctahEnih?from=space_search<br>",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Event-18",
      "label": "backoff",
      "details": "常见的backoff类型<br>pdRPC<br>regionMiss<br>serverBusy<br>tikvRPC<br>txnLock<br>txnLockFast<br>updateLeader",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "Index-24,",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-32",
      "label": "region merge 速度慢",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "Config-15,Config-16",
      "link": "",
      "picture": "=DISPIMG(\"ID_5D74875A8A264D3C9FB68F722C1160C3\",1)"
    },
    {
      "id": "Page-34",
      "label": "读取瓶颈分析",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": "=DISPIMG(\"ID_8D7D703E95EE46468F10B78A33C0E777\",1)"
    },
    {
      "id": "Page-35",
      "label": "TiKV 相关监控",
      "details": "",
      "originalSize": 20.0,
      "originalDegree": 0.0,
      "source": "",
      "target": "",
      "link": "",
      "picture": "=DISPIMG(\"ID_03C3734BF091406A86826B77A5C657AB\",1)"
    },
    {
      "id": "Page-36",
      "label": "新特性解析丨TiDB 资源管控的设计思路与场景解析",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "",
      "target": "",
      "link": "https://tidb.net/blog/67d82266",
      "picture": ""
    },
    {
      "id": "Config-39",
      "label": "tidb_enable_resource_control",
      "details": "since:6.6.0<br>detail:从 v7.0.0 开始，tidb_enable_resource_control 和 resource-control.enabled 开关都被默认打开。<br>",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-36",
      "target": "",
      "link": "https://pingkai.cn/docs/tidb/stable/system-variable-reference/#tidb_enable_resource_control",
      "picture": ""
    },
    {
      "id": "Page-37",
      "label": "OOM-相关参数",
      "details": "oom-action参数已经被另一个参数替代<br><br>tidb_mem_oom_action<br>触发条件：<br>1、mem-quota-query达到<br>2、没有临时磁盘使用<br>（1）、oom-use-tmp-storage #设置为了true<br>（2）、tmp-storage-path #使用磁盘的位置<br>（3）、tmp-storage-quota #临时磁盘使用量，单位byte<br><br>",
      "originalSize": "",
      "originalDegree": "",
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-40",
      "label": "tidb_mem_oom_action",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-37",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-41",
      "label": "mem-quota-query",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-37",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-42",
      "label": "oom-use-tmp-storage",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-37",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-43",
      "label": "tmp-storage-path",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-37",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Config-44",
      "label": "tmp-storage-quota",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-37",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "Page-38",
      "label": "统计信息加载",
      "details": "默认情况下，列的统计信息占用空间大小不同，TiDB 对统计信息的加载方式也会不同：<br><br>对于 count、distinctCount、nullCount 等占用空间较小的统计信息，只要有数据更新，TiDB 就会自动将对应的统计信息加载进内存供 SQL 优化阶段使用。<br>对于直方图、TopN、CMSketch 等占用空间较大的统计信息，为了确保 SQL 执行的性能，TiDB 会按需进行异步加载。例如，对于直方图，只有当某条 SQL 语句的优化阶段使用到了某列的直方图统计信息时，TiDB 才会将该列的直方图信息加载到内存。按需异步加载的优势是统计信息加载不会影响到 SQL 执行的性能，但在 SQL 优化时有可能使用不完整的统计信息。<br>从 v5.4.0 开始，TiDB 引入了统计信息同步加载的特性，支持执行当前 SQL 语句时将直方图、TopN、CMSketch 等占用空间较大的统计信息同步加载到内存，提高该 SQL 语句优化时统计信息的完整性。",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-28",
      "target": "",
      "link": "https://pingkai.cn/docs/tidb/stable/statistics/#%E5%8A%A0%E8%BD%BD%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF",
      "picture": ""
    },
    {
      "id": "Config-45",
      "label": "tidb_stats_load_sync_wait",
      "details": "",
      "originalSize": "",
      "originalDegree": "",
      "source": "Page-38",
      "target": "",
      "link": "",
      "picture": ""
    },
    {
      "id": "",
      "label": "lite-init-stats",
      "details": "从 v7.1.0 开始，TiDB 引入了配置参数 lite-init-stats，用于控制是否开启轻量级的统计信息初始化。<br><br>当 lite-init-stats 设置为 true 时，统计信息初始化时列和索引的直方图、TopN、Count-Min Sketch 均不会加载到内存中。<br>当 lite-init-stats 设置为 false 时，统计信息初始化时索引和主键的直方图、TopN、Count-Min Sketch 会被加载到内存中，非主键列的直方图、TopN、Count-Min Sketch 不会加载到内存中。当优化器需要某一索引或者列的直方图、TopN、Count-Min Sketch 时，这些统计信息会被同步或异步加载到内存中。<br>lite-init-stats 的默认值为 true，即开启轻量级的统计信息初始化。将 lite-init-stats 设置为 true 可以加速统计信息初始化，避免加载不必要的统计信息，从而减少 TiDB 的内存使用。",
      "originalSize": "",
      "originalDegree": "",
      "source": "",
      "target": "",
      "link": "",
      "picture": ""
    }
  ],
  "edges": [
    {
      "source": "Page-00",
      "target": "Page-01"
    },
    {
      "source": "Page-01",
      "target": "Config-13"
    },
    {
      "source": "Event-05",
      "target": "Config-13"
    },
    {
      "source": "Page-01",
      "target": "Config-14"
    },
    {
      "source": "Event-05",
      "target": "Config-14"
    },
    {
      "source": "Page-01",
      "target": "Config-15"
    },
    {
      "source": "Page-01",
      "target": "Config-16"
    },
    {
      "source": "Page-01",
      "target": "Config-17"
    },
    {
      "source": "Page-01",
      "target": "Index-01"
    },
    {
      "source": "Page-01",
      "target": "Config-01"
    },
    {
      "source": "Page-01",
      "target": "Index-02"
    },
    {
      "source": "Page-00",
      "target": "Page-02"
    },
    {
      "source": "Page-02",
      "target": "Page-03"
    },
    {
      "source": "Page-02",
      "target": "Config-02"
    },
    {
      "source": "Page-02",
      "target": "Index-03"
    },
    {
      "source": "Page-02",
      "target": "Index-04"
    },
    {
      "source": "Page-02",
      "target": "Index-05"
    },
    {
      "source": "Page-02",
      "target": "Index-06"
    },
    {
      "source": "Page-02",
      "target": "Index-07"
    },
    {
      "source": "Index-07",
      "target": "Index-43"
    },
    {
      "source": "Page-15",
      "target": "Index-44"
    },
    {
      "source": "Page-02",
      "target": "Index-08"
    },
    {
      "source": "Page-00",
      "target": "Page-03"
    },
    {
      "source": "Page-03",
      "target": "Index-07"
    },
    {
      "source": "Page-03",
      "target": "Index-08"
    },
    {
      "source": "Page-03",
      "target": "Page-06"
    },
    {
      "source": "Page-03",
      "target": "Config-03"
    },
    {
      "source": "Page-03",
      "target": "Config-04"
    },
    {
      "source": "Page-03",
      "target": "Config-05"
    },
    {
      "source": "Page-03",
      "target": "Config-06"
    },
    {
      "source": "Page-03",
      "target": "Error-01"
    },
    {
      "source": "Page-03",
      "target": "Page-04"
    },
    {
      "source": "Page-04",
      "target": "Error-02"
    },
    {
      "source": "Page-03",
      "target": "Page-05"
    },
    {
      "source": "Page-05",
      "target": "Error-03"
    },
    {
      "source": "Page-05",
      "target": "Error-04"
    },
    {
      "source": "Page-00",
      "target": "Page-06"
    },
    {
      "source": "Page-06",
      "target": "Config-03"
    },
    {
      "source": "Page-06",
      "target": "Config-04"
    },
    {
      "source": "Page-06",
      "target": "Config-05"
    },
    {
      "source": "Page-06",
      "target": "Config-06"
    },
    {
      "source": "Page-06",
      "target": "Page-07"
    },
    {
      "source": "Page-06",
      "target": "Note-02"
    },
    {
      "source": "Page-06",
      "target": "Note-03"
    },
    {
      "source": "Note-02",
      "target": "Page-07"
    },
    {
      "source": "Page-07",
      "target": "Page-08"
    },
    {
      "source": "Page-07",
      "target": "Note-04"
    },
    {
      "source": "Note-04",
      "target": "Note-05"
    },
    {
      "source": "Page-07",
      "target": "Note-05"
    },
    {
      "source": "Note-05",
      "target": "Note-06"
    },
    {
      "source": "Page-07",
      "target": "Note-06"
    },
    {
      "source": "Note-06",
      "target": "Note-07"
    },
    {
      "source": "Page-07",
      "target": "Note-07"
    },
    {
      "source": "Note-07",
      "target": "Config-03"
    },
    {
      "source": "Note-07",
      "target": "Config-04"
    },
    {
      "source": "Note-07",
      "target": "Config-05"
    },
    {
      "source": "Note-07",
      "target": "Config-06"
    },
    {
      "source": "Note-07",
      "target": "Note-08"
    },
    {
      "source": "Page-07",
      "target": "Note-08"
    },
    {
      "source": "Note-08",
      "target": "Note-09"
    },
    {
      "source": "Note-08",
      "target": "Note-11"
    },
    {
      "source": "Page-07",
      "target": "Note-09"
    },
    {
      "source": "Note-09",
      "target": "Note-10"
    },
    {
      "source": "Page-07",
      "target": "Note-10"
    },
    {
      "source": "Note-03",
      "target": "Page-08"
    },
    {
      "source": "Page-08",
      "target": "Note-04"
    },
    {
      "source": "Page-08",
      "target": "Note-05"
    },
    {
      "source": "Page-08",
      "target": "Note-06"
    },
    {
      "source": "Page-08",
      "target": "Note-11"
    },
    {
      "source": "Note-11",
      "target": "Note-12"
    },
    {
      "source": "Page-08",
      "target": "Note-12"
    },
    {
      "source": "Note-12",
      "target": "Note-15"
    },
    {
      "source": "Page-08",
      "target": "Note-13"
    },
    {
      "source": "Page-08",
      "target": "Note-14"
    },
    {
      "source": "Page-08",
      "target": "Note-15"
    },
    {
      "source": "Note-15",
      "target": "Note-07"
    },
    {
      "source": "Page-08",
      "target": "Note-16"
    },
    {
      "source": "Note-16",
      "target": "Index-03"
    },
    {
      "source": "Note-16",
      "target": "Index-04"
    },
    {
      "source": "Note-16",
      "target": "Index-05"
    },
    {
      "source": "Page-00",
      "target": "Page-09"
    },
    {
      "source": "Page-09",
      "target": "Page-10"
    },
    {
      "source": "Page-09",
      "target": "Note-17"
    },
    {
      "source": "Page-09",
      "target": "Note-18"
    },
    {
      "source": "Page-00",
      "target": "Page-10"
    },
    {
      "source": "Page-10",
      "target": "Error-5"
    },
    {
      "source": "Page-10",
      "target": "Index-09"
    },
    {
      "source": "Page-10",
      "target": "Event-01"
    },
    {
      "source": "Page-10",
      "target": "Event-02"
    },
    {
      "source": "Event-01",
      "target": "Theory-01"
    },
    {
      "source": "Event-01",
      "target": "Event-03"
    },
    {
      "source": "Event-03",
      "target": "Index-10"
    },
    {
      "source": "Event-03",
      "target": "Index-11"
    },
    {
      "source": "Event-03",
      "target": "Log-01"
    },
    {
      "source": "Event-03",
      "target": "Config-06"
    },
    {
      "source": "Event-03",
      "target": "Config-07"
    },
    {
      "source": "Event-03",
      "target": "Config-08"
    },
    {
      "source": "Event-01",
      "target": "Event-04"
    },
    {
      "source": "Event-04",
      "target": "Index-12"
    },
    {
      "source": "Event-04",
      "target": "Index-13"
    },
    {
      "source": "Index-13",
      "target": "Log-01"
    },
    {
      "source": "Event-04",
      "target": "Config-09"
    },
    {
      "source": "Event-04",
      "target": "Config-10"
    },
    {
      "source": "Event-04",
      "target": "Config-11"
    },
    {
      "source": "Event-04",
      "target": "Config-12"
    },
    {
      "source": "Event-01",
      "target": "Event-05"
    },
    {
      "source": "Event-05",
      "target": "Index-01"
    },
    {
      "source": "Event-05",
      "target": "Index-14"
    },
    {
      "source": "Event-05",
      "target": "Index-15"
    },
    {
      "source": "Event-05",
      "target": "Index-17"
    },
    {
      "source": "Event-05",
      "target": "Config-18"
    },
    {
      "source": "Event-05",
      "target": "Config-19"
    },
    {
      "source": "Event-02",
      "target": "Event-06"
    },
    {
      "source": "Event-06",
      "target": "Event-10"
    },
    {
      "source": "Event-10",
      "target": "Config-20"
    },
    {
      "source": "Event-10",
      "target": "Index-18"
    },
    {
      "source": "Event-10",
      "target": "Index-19"
    },
    {
      "source": "Event-10",
      "target": "Index-20"
    },
    {
      "source": "Event-10",
      "target": "Index-21"
    },
    {
      "source": "Event-10",
      "target": "Index-22"
    },
    {
      "source": "Event-10",
      "target": "Index-23"
    },
    {
      "source": "Event-10",
      "target": "Index-24"
    },
    {
      "source": "Event-10",
      "target": "Log-02"
    },
    {
      "source": "Event-06",
      "target": "Event-07"
    },
    {
      "source": "Event-07",
      "target": "Page-24"
    },
    {
      "source": "Event-06",
      "target": "Event-08"
    },
    {
      "source": "Event-08",
      "target": "Index-25"
    },
    {
      "source": "Index-25",
      "target": "Log-03"
    },
    {
      "source": "Event-08",
      "target": "Log-03"
    },
    {
      "source": "Page-00",
      "target": "Page-11"
    },
    {
      "source": "Page-11",
      "target": "Index-26"
    },
    {
      "source": "Page-11",
      "target": "Index-27"
    },
    {
      "source": "Page-11",
      "target": "Index-28"
    },
    {
      "source": "Page-11",
      "target": "Index-29"
    },
    {
      "source": "Page-11",
      "target": "Event-09"
    },
    {
      "source": "Page-11",
      "target": "Event-15"
    },
    {
      "source": "Event-15",
      "target": "Index-07"
    },
    {
      "source": "Page-11",
      "target": "Index-30"
    },
    {
      "source": "Event-09",
      "target": "Event-14"
    },
    {
      "source": "Event-14",
      "target": "Event-07"
    },
    {
      "source": "Event-14",
      "target": "Event-08"
    },
    {
      "source": "Event-14",
      "target": "Event-12"
    },
    {
      "source": "Event-12",
      "target": "Index-31"
    },
    {
      "source": "Event-12",
      "target": "Event-11"
    },
    {
      "source": "Event-11",
      "target": "Config-25"
    },
    {
      "source": "Event-12",
      "target": "Config-21"
    },
    {
      "source": "Event-14",
      "target": "Event-13"
    },
    {
      "source": "Event-13",
      "target": "Index-32"
    },
    {
      "source": "Event-08",
      "target": "Index-33"
    },
    {
      "source": "Event-08",
      "target": "Index-57"
    },
    {
      "source": "Event-14",
      "target": "Event-16"
    },
    {
      "source": "Event-16",
      "target": "Index-34"
    },
    {
      "source": "Event-15",
      "target": "Index-35"
    },
    {
      "source": "Event-15",
      "target": "Index-36"
    },
    {
      "source": "Event-15",
      "target": "Index-37"
    },
    {
      "source": "Page-00",
      "target": "Page-12"
    },
    {
      "source": "Page-12",
      "target": "Page-13"
    },
    {
      "source": "Page-12",
      "target": "Page-14"
    },
    {
      "source": "Page-14",
      "target": "Index-07"
    },
    {
      "source": "Page-14",
      "target": "Index-46"
    },
    {
      "source": "Page-14",
      "target": "Index-47"
    },
    {
      "source": "Page-14",
      "target": "Index-48"
    },
    {
      "source": "Page-14",
      "target": "Index-49"
    },
    {
      "source": "Page-14",
      "target": "Index-50"
    },
    {
      "source": "Page-14",
      "target": "Config-26"
    },
    {
      "source": "Page-14",
      "target": "Config-25"
    },
    {
      "source": "Page-12",
      "target": "Page-15"
    },
    {
      "source": "Page-15",
      "target": "Index-27"
    },
    {
      "source": "Page-15",
      "target": "Index-07"
    },
    {
      "source": "Page-15",
      "target": "Index-06"
    },
    {
      "source": "Page-12",
      "target": "Page-16"
    },
    {
      "source": "Page-16",
      "target": "Index-51"
    },
    {
      "source": "Page-13",
      "target": "Index-38"
    },
    {
      "source": "Page-13",
      "target": "Index-39"
    },
    {
      "source": "Page-13",
      "target": "Index-40"
    },
    {
      "source": "Page-13",
      "target": "Page-17"
    },
    {
      "source": "Page-15",
      "target": "Index-41"
    },
    {
      "source": "Page-15",
      "target": "Index-42"
    },
    {
      "source": "Page-15",
      "target": "Index-45"
    },
    {
      "source": "Page-15",
      "target": "Config-22"
    },
    {
      "source": "Page-15",
      "target": "Config-23"
    },
    {
      "source": "Page-15",
      "target": "Config-24"
    },
    {
      "source": "Page-00",
      "target": "Page-18"
    },
    {
      "source": "Page-18",
      "target": "Log-04"
    },
    {
      "source": "Page-18",
      "target": "Note-19"
    },
    {
      "source": "Page-18",
      "target": "Index-52"
    },
    {
      "source": "Page-18",
      "target": "Index-53"
    },
    {
      "source": "Page-18",
      "target": "Index-54"
    },
    {
      "source": "Page-18",
      "target": "Index-55"
    },
    {
      "source": "Page-18",
      "target": "Index-56"
    },
    {
      "source": "Page-00",
      "target": "Page-19"
    },
    {
      "source": "Page-19",
      "target": "Event-08"
    },
    {
      "source": "Page-19",
      "target": "Index-58"
    },
    {
      "source": "Page-19",
      "target": "Index-59"
    },
    {
      "source": "Page-19",
      "target": "Event-17"
    },
    {
      "source": "Page-00",
      "target": "Page-20"
    },
    {
      "source": "Page-20",
      "target": "Index-59"
    },
    {
      "source": "Page-00",
      "target": "Page-21"
    },
    {
      "source": "Page-21",
      "target": "Index-60"
    },
    {
      "source": "Page-22",
      "target": "Page-23"
    },
    {
      "source": "Page-23",
      "target": "Index-52"
    },
    {
      "source": "Page-23",
      "target": "Index-53"
    },
    {
      "source": "Page-23",
      "target": "Index-54"
    },
    {
      "source": "Page-23",
      "target": "Index-55"
    },
    {
      "source": "Page-23",
      "target": "Index-56"
    },
    {
      "source": "Page-22",
      "target": "Page-24"
    },
    {
      "source": "Page-24",
      "target": "Index-19"
    },
    {
      "source": "Page-24",
      "target": "Index-31"
    },
    {
      "source": "Page-24",
      "target": "Index-60"
    },
    {
      "source": "Page-24",
      "target": "Note-20"
    },
    {
      "source": "Page-24",
      "target": "Index-61"
    },
    {
      "source": "Page-24",
      "target": "Index-62"
    },
    {
      "source": "Page-24",
      "target": "Index-63"
    },
    {
      "source": "Page-24",
      "target": "Index-64"
    },
    {
      "source": "Page-24",
      "target": "Index-65"
    },
    {
      "source": "Page-24",
      "target": "Index-66"
    },
    {
      "source": "Page-24",
      "target": "Index-67"
    },
    {
      "source": "Page-22",
      "target": "Page-25"
    },
    {
      "source": "Page-25",
      "target": "Index-38"
    },
    {
      "source": "Page-25",
      "target": "Index-44"
    },
    {
      "source": "Page-25",
      "target": "Config-27"
    },
    {
      "source": "Page-25",
      "target": "Config-28"
    },
    {
      "source": "Page-25",
      "target": "Index-69"
    },
    {
      "source": "Page-26",
      "target": "Config-37"
    },
    {
      "source": "Page-26",
      "target": "Config-38"
    },
    {
      "source": "Doc-01",
      "target": "Doc-02"
    },
    {
      "source": "Doc-02",
      "target": "Doc-03"
    },
    {
      "source": "Page-28",
      "target": "Config-29"
    },
    {
      "source": "Page-28",
      "target": "Config-30"
    },
    {
      "source": "Page-28",
      "target": "Config-31"
    },
    {
      "source": "Page-28",
      "target": "Config-32"
    },
    {
      "source": "Page-29",
      "target": "Config-33"
    },
    {
      "source": "Page-29",
      "target": "Config-34"
    },
    {
      "source": "Event-18",
      "target": "Index-24"
    },
    {
      "source": "Page-32",
      "target": "Config-15"
    },
    {
      "source": "Page-32",
      "target": "Config-16"
    },
    {
      "source": "Page-36",
      "target": "Config-39"
    },
    {
      "source": "Page-37",
      "target": "Config-40"
    },
    {
      "source": "Page-37",
      "target": "Config-41"
    },
    {
      "source": "Page-37",
      "target": "Config-42"
    },
    {
      "source": "Page-37",
      "target": "Config-43"
    },
    {
      "source": "Page-37",
      "target": "Config-44"
    },
    {
      "source": "Page-28",
      "target": "Page-38"
    },
    {
      "source": "Page-38",
      "target": "Config-45"
    }
  ]
}